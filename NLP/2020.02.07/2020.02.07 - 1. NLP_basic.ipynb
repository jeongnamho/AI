{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고자료 : [데이터사이언스 스쿨](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.008841Z",
     "start_time": "2020-02-10T00:09:07.720170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from konlpy.tag import Twitter  # konlpy 설치필요\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma \n",
    "from konlpy.tag import Twitter\n",
    "from pprint import pprint\n",
    "import nltk   # 설치 필요\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS      # wordcloud 설치필요\n",
    "from gensim import corpora, models      # 설치필요\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "from wordcloud import ImageColorGenerator\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Char코드 기반의 NLP\n",
    "### 유니코드\n",
    "- 각 글자들은 유니코드를 갖고 있음.\n",
    "- 코드값의 범위가 있기 때문에 범위를 보고 대충 어떤 언어인지 알 수 있음\n",
    "- 영어와 한글은 범위가 다르기 때문에 범위만 보고 분류가 가능하지만\n",
    "- 같은 알파벳을 쓰는 영어와 스페인어는 분류가 불가\n",
    "- 유니코드는 총 65536개이기 때문에 어떤 문자든 한 글자당 65536차원 배열을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Bayes’ theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$ :\n",
    "\n",
    "\\begin{align}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}\n",
    "                                 \n",
    "\\end{align}\n",
    "Using the naive conditional independence assumption that\n",
    "\n",
    "\\begin{align}\n",
    "P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),\n",
    "\\end{align}\n",
    "\n",
    "for all , this relationship is simplified to\n",
    "\n",
    "\\begin{align}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}\n",
    "\\end{align}\n",
    "\n",
    "Since $P(x_1, \\dots, x_n)$ is constant given the input, we can use the following classification rule:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\n",
    "\\end{aligned}\n",
    "\\end{align} \n",
    "\n",
    "and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$\n",
    "; the former is then the relative frequency of class $y$ in the training set.\n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \\mid y)$\n",
    ".\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)\n",
    "\n",
    "Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from `predict_proba` are not to be taken too seriously.\n",
    "\n",
    "**References**:\n",
    "\n",
    "H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 GaussianNB\n",
    "[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "\\begin{align}\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n",
    "\\end{align}\n",
    "\n",
    "The parameters $\\sigma_y$ and $\\mu_y$ are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T08:37:50.553947Z",
     "start_time": "2020-02-08T08:37:50.532007Z"
    }
   },
   "source": [
    "### 1.1 GaussianNB 분류기\n",
    "- 평균(mean), 표준편차(standard deviation)을 이용\n",
    "    - 데이터들끼리 거리를 모두 비교하는 K-nearest Neighbor) 방법도 있지만 overfitting이 심해서 잘 안쓰고 Gaussian을 씀\n",
    "\n",
    "- 정규화해서\n",
    "    - 샘플데이터를 평균으로 모델링해서\n",
    "    - class간에 평균, 표준편차를 이용해서\n",
    "    - 데이터가 어느 class에 가까운지 비교하기\n",
    "    - 정규화해서 mean, standard deviation을 구하면 -> 확률값을 구할 수 있음\n",
    "    - class간 속할 확률을 구해서, 큰 쪽으로 분류하면 됌\n",
    "    \n",
    "- 학습시킨다는 의미\n",
    "    - 평균, 분산을 구한다는 의미\n",
    "    - 데이터가 1개면 그 데이터가 평균이 됌\n",
    "    - K-nearest Neighbors 학습이 필요 없음\n",
    "    \n",
    "- y 값\n",
    "    - 딥러닝에서 err를 계산하기 위해 반드시 숫자였어야 함.\n",
    "    - 이 분류기에서는 참고하는 역할만 하기 때문에 숫자가 아니어도 됌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.127318Z",
     "start_time": "2020-02-10T00:09:14.009610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a8ea387748>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAUSklEQVR4nO3df4xd9Xnn8fcnBvPDkJAtAwZsx6ixViWIhNXUbX5CBSHGcePSJhFOmgSa1qUFbSvttiFFItrdIm0UtQ2QNNRtSINKwi6lLpZwEiAsAqLSMCB+mBhYiyaLYwITIBhjU7B59o97UYbxHXtm7vXcmXveL+lqzvme75znObL9meNzz9yTqkKSNPje0O8GJEkzw8CXpIYw8CWpIQx8SWoIA1+SGuKgfjewL0cffXQtXbq0321I0pxx7733/rSqhjptm9WBv3TpUkZGRvrdhiTNGUl+NNE2L+lIUkMY+JLUEAa+JDWEgS9JDWHgS1JDzOq7dCSpKV55BW68EW67DRYtgvPOg+OP722NrgM/yWLgGmAh8CqwrqouHzcnwOXASmAncF5V3ddtbUkaBDt3wnvfC489Bjt2wCGHwGWXwU03wemn965OLy7p7Ab+S1X9EvCrwIVJTho352xgWfu1FvhKD+pK0kC48krYvLkV9gD//u+tHwJr1sCrr/auTteBX1VPvna2XlUvAJuBE8ZNWw1cUy13A0clOa7b2pI0CK69Fnbt2nv8hRdaPwh6padv2iZZCpwK/Ou4TScAT4xZ38rePxRe28faJCNJRkZHR3vZniTNSocc0nm8auJt09GzwE9yBHAD8MdVtX385g7f0vFRW1W1rqqGq2p4aKjjx0FI0kD5/d+HBQteP5bAkiXw1rf2rk5PAj/JwbTC/tqq+qcOU7YCi8esLwK29aK2JM11558PH/oQHHZY63XkkTA0BOvX97ZOL+7SCfBVYHNV/eUE0zYAFyW5DvgV4PmqerLb2pI0CObNg298AzZtgu99DxYuhLPPhvnze1unF/fhvxv4BPBQkvvbY38GLAGoqquAjbRuydxC67bM83tQV5IGysknt14HSteBX1V30fka/dg5BVzYbS1J0vT50QqS1BAGviQ1hIEvSQ1h4EtSQxj4ktQQBr4kNYSBL0kNYeBLUkMY+JLUEAa+JDWEgS9JDWHgS1JDGPiS1BAGviQ1hIEvSQ3Rq0ccXp3k6SSbJth+epLnk9zffl3ai7qSpMnrxROvAP4e+BJwzT7m3FlVq3pUT5I0RT05w6+qO4Bne7EvSdKBMZPX8N+Z5IEk30rytokmJVmbZCTJyOjo6Ay2J0mDbaYC/z7gLVX1duBK4J8nmlhV66pquKqGh4aGZqg9SRp8MxL4VbW9qna0lzcCByc5eiZqS5JaZiTwkyxMkvby8nbdZ2aitiSppSd36ST5JnA6cHSSrcDngIMBquoq4MPAHyTZDewCzq2q6kVtSdLk9CTwq2rNfrZ/idZtm5KkPvE3bSWpIQx8SWoIA1+SGsLAl6SGMPAlqSEMfElqCANfkhrCwJekhjDwJakhDHxJaggDX5IawsCXpIYw8CWpIQx8SWoIA1+SGqInn4cvaW7ZvRtuugkefBCWLYNzzoFDDul3V9Pz5JNw/fXw4ovwwQ/CKaf0u6PZK7148FSSq4FVwNNVdXKH7QEuB1YCO4Hzquq+/e13eHi4RkZGuu5P0s899xy8612wdWsrJBcsgDe9Cf7lX2Dx4n53NzU33ACf+ARUwSuvwPz58OlPwxVXQOuhqs2T5N6qGu60rVeXdP4eWLGP7WcDy9qvtcBXelRX0hT96Z/C44/Djh2toNyxA37yE/i93+t3Z1OzfTt88pOwaxe89BLs2dNa/trX4Pbb+93d7NSTwK+qO4Bn9zFlNXBNtdwNHJXkuF7UljQ1118PL7/8+rE9e+C73917fDa7+WaYN2/v8Z074R/+Yeb7mQtm6k3bE4AnxqxvbY/tJcnaJCNJRkZHR2ekOUlqgpkK/E5X0zq+eVBV66pquKqGh4aGDnBbUvN89KOta91jzZsHZ5yx9/hsdtZZrf+ZjHf44fDbvz3z/cwFMxX4W4GxbwctArbNUG1JY3z+83DiiXDEEa03No84Ao49Fv72b/vd2dS88Y1wzTVw2GFw6KFw0EGt5fPPh9NP73d3s9NM3Za5AbgoyXXArwDPV9WTM1Rb0hhvfjNs2jQYt2X+1m+17jjytszJ6dVtmd8ETgeOBp4CPgccDFBVV7Vvy/wSrTt5dgLnV9V+77f0tkxJmpp93ZbZkzP8qlqzn+0FXNiLWpKk6fGjFSSpIQx8SWoIA1+SGsLAl6SGMPAlqSEMfElqCANfkhrCwJekhjDwJakhDHxJaggDX5IawsCXpIYw8CWpIQx8SWoIA1+SGqIngZ9kRZJHk2xJcnGH7acneT7J/e3Xpb2oK0mavK4fgJJkHvBl4P20nl17T5INVfWDcVPvrKpV3daTJE1PL87wlwNbqurxqnoZuA5Y3YP9SpJ6qBeBfwLwxJj1re2x8d6Z5IEk30ryth7UlSRNQS+eaZsOY+OfjH4f8Jaq2pFkJfDPwLKOO0vWAmsBlixZ0oP2JEnQmzP8rcDiMeuLgG1jJ1TV9qra0V7eCByc5OhOO6uqdVU1XFXDQ0NDPWhPkgS9Cfx7gGVJTkwyHzgX2DB2QpKFSdJeXt6u+0wPakuSJqnrSzpVtTvJRcB3gHnA1VX1cJIL2tuvAj4M/EGS3cAu4NyqGn/ZR5J0AGU25+7w8HCNjIz0uw1JmjOS3FtVw522+Zu2ktQQBr4kNYSBL0kNYeBLUkMY+JLUEAa+JDWEgS9JDWHgS1JDGPiS1BAGviQ1hIEvSQ1h4EtSQxj4ktQQBr4kNYSBL0kNYeBLUkP0JPCTrEjyaJItSS7usD1JrmhvfzDJf+pFXUnS5HUd+EnmAV8GzgZOAtYkOWnctLOBZe3XWuAr3daVJE1NL87wlwNbqurxqnoZuA5YPW7OauCaarkbOCrJcT2oLUmapF4E/gnAE2PWt7bHpjoHgCRrk4wkGRkdHe1Be5Ik6E3gp8PY+CejT2ZOa7BqXVUNV9Xw0NBQ181Jklp6EfhbgcVj1hcB26YxR5J0APUi8O8BliU5Mcl84Fxgw7g5G4BPtu/W+VXg+ap6sge1JUmTdFC3O6iq3UkuAr4DzAOurqqHk1zQ3n4VsBFYCWwBdgLnd1tXkjQ1XQc+QFVtpBXqY8euGrNcwIW9qCVJmh5/01aSGsLAl6SGMPAlqSEMfElqCANfkhrCwJekhjDwJakhDHxJaggDX5IawsCXpIYw8CWpIQx8SWoIA1+SGsLAl6SGMPAlqSG6+jz8JP8B+F/AUuCHwEer6rkO834IvADsAXZX1XA3dSVJU9ftGf7FwHerahnw3fb6RH6tqt5h2EtSf3Qb+KuBr7eXvw78Rpf7kyQdIN0G/rGvPYy8/fWYCeYVcHOSe5Os7bKmJGka9nsNP8mtwMIOmy6ZQp13V9W2JMcAtyR5pKrumKDeWmAtwJIlS6ZQQpK0L/sN/Ko6c6JtSZ5KclxVPZnkOODpCfaxrf316STrgeVAx8CvqnXAOoDh4eHa/yFIkiaj20s6G4BPtZc/Bdw4fkKSBUmOfG0ZOAvY1GVdSdIUdRv4/xN4f5L/C7y/vU6S45NsbM85FrgryQPA94GbqurbXdaVJE1RV/fhV9UzwBkdxrcBK9vLjwNv76aOJKl7/qatJDWEgS9JDWHgS1JDGPiS1BAGviQ1hIEvSQ1h4EtSQxj4ktQQBr4kNYSBL0kNYeBLUkMY+JLUEAa+JDWEgS9JDWHgS1JDdPV5+DrAnnkG7rgD3vQmeN/74KA5+sdVBffdB//2b3DqqfCLv9jvjqRG6uoMP8lHkjyc5NUkw/uYtyLJo0m2JLm4m5qN8Rd/AYsWwXnnwTnntJYffLDfXU3ds8/C8DCcdhp8+tNw8smwZg3s3j3BN1wLLKX1V3Npe11SL3R7SWcT8JtM8EBygCTzgC8DZwMnAWuSnNRl3cH2ve/BpZfCSy/B9u2t11NPwQc+AHv29Lu7qTnvPHjoIXjxxdZxvPQSbNgAf/VXHSZfC6wFfgRU++taDH2pN7oK/KraXFWP7mfacmBLVT1eVS8D1wGru6k78K66Cnbt2nv8xRfhrrtmvp/p2rEDvv1teOWV14/v3Al//dcdvuESYOe4sZ3tcUndmok3bU8AnhizvrU91lGStUlGkoyMjo4e8OZmpeeea133Hi+BF16Y+X6m66WXWj13smNHh8H/N8GOJhqXNBX7DfwktybZ1OE12bP0Tv/iO6RZe0PVuqoarqrhoaGhSZYYMB/+MCxYsPf4yy/De94z8/1M1y/8Aixduvf4QQfBqlUdvmHJBDuaaFzSVOw38KvqzKo6ucPrxknW2AosHrO+CNg2nWYb42Mfg1NO+Xnov+ENcPjhrTdyjzqqv71NRQJf+xoccQTMn98aO+yw1g+CP//zDt9wGXD4uLHD2+OSujUT9/ndAyxLciLwY+Bc4GMzUHfumj8fbr8drr8e/vEfWwF5wQWtu13mmne9CzZtal2z37wZ3vte+N3fhTe/ucPkj7e/XkLrMs4SWmH/8Q5zJU1VqtO14sl+c3IOcCUwBPwMuL+qPpDkeODvqmple95K4IvAPODqqprUKdvw8HCNjIxMuz9Japok91ZVx7PDrs7wq2o9sL7D+DZg5Zj1jcDGbmpJkrrjRytIUkMY+JLUEAa+JDWEgS9JDWHgS1JDGPiS1BAGviQ1hIEvSQ1h4EtSQxj4ktQQBr4kNYSBL0kNYeBLUkMY+JLUEAa+JDWEgS9JDdFV4Cf5SJKHk7yaZMLn7yX5YZKHktyfxEdYSVIfdPtM203AbwJ/M4m5v1ZVP+2yniRpmrp9xOFmgCS96UaSdMDM1DX8Am5Ocm+StfuamGRtkpEkI6OjozPUniQNvv2e4Se5FVjYYdMlVXXjJOu8u6q2JTkGuCXJI1V1R6eJVbUOWAcwPDxck9y/JGk/9hv4VXVmt0Wqalv769NJ1gPLgY6BL0k6MA74JZ0kC5Ic+doycBatN3slSTOo29syz0myFXgncFOS77THj0+ysT3tWOCuJA8A3wduqqpvd1NXkjR13d6lsx5Y32F8G7Cyvfw48PZu6kiSuudv2kpSQxj4ktQQBr4kNYSBL0kNYeBLUkMY+JLUEAa+JDWEgS9JDWHgS1JDGPiS1BAGviQ1hIEvSQ1h4EtSQxj4ktQQgxn4P/gB3Hkn7NjR704kadbo9gEoX0jySJIHk6xPctQE81YkeTTJliQXd1Nzn7Ztg3e8A375l2HVKjjmGLjiigNWTpLmkm7P8G8BTq6qU4DHgM+On5BkHvBl4GzgJGBNkpO6rNvZBz8ImzbBzp2wfTvs2gWf/SzcdtsBKSdJc0lXgV9VN1fV7vbq3cCiDtOWA1uq6vGqehm4DljdTd2ONm+Gxx6DPXteP75zJ1x+ec/LSdJc08tr+L8DfKvD+AnAE2PWt7bHOkqyNslIkpHR0dHJV3/mGThogic2/uQnk9+PJA2o/T7TNsmtwMIOmy6pqhvbcy4BdgPXdtpFh7GaqF5VrQPWAQwPD084by+nngq7d+89fuih8Ou/PundSNKg2m/gV9WZ+9qe5FPAKuCMquoU0FuBxWPWFwHbptLkpCxYAF/4AvzJn7Qu40Ar7BcuhIsu6nk5SZpr9hv4+5JkBfAZ4LSq2jnBtHuAZUlOBH4MnAt8rJu6E/rDP4S3vQ2++MXWZZxVq+DCC+GojjcPSVKjdBX4wJeAQ4BbkgDcXVUXJDke+LuqWllVu5NcBHwHmAdcXVUPd1l3Yqed1npJkl6nq8CvqrdOML4NWDlmfSOwsZtakqTuDOZv2kqS9mLgS1JDGPiS1BAGviQ1hIEvSQ2Rzr8rNTskGQV+NM1vPxr4aQ/b6adBOZZBOQ7wWGajQTkO6O5Y3lJVQ502zOrA70aSkaoa7ncfvTAoxzIoxwEey2w0KMcBB+5YvKQjSQ1h4EtSQwxy4K/rdwM9NCjHMijHAR7LbDQoxwEH6FgG9hq+JOn1BvkMX5I0hoEvSQ0x0IGf5H8keTDJ/Ulubn9s85yU5AtJHmkfz/okc/JD/pN8JMnDSV5NMuduoUuyIsmjSbYkubjf/UxXkquTPJ1kU7976VaSxUn+T5LN7b9bf9TvnqYjyaFJvp/kgfZx/Lee1xjka/hJ3lhV29vL/xk4qaou6HNb05LkLOC29vMFPg9QVZ/pc1tTluSXgFeBvwH+a1WN9LmlSUsyD3gMeD+tJ7ndA6ypqh/0tbFpSPI+YAdwTVWd3O9+upHkOOC4qrovyZHAvcBvzLU/l7QeKrKgqnYkORi4C/ijqrq7VzUG+gz/tbBvW8A+nqU721XVzVX12kN776b1qMg5p6o2V9Wj/e5jmpYDW6rq8ap6GbgOWN3nnqalqu4Anu13H71QVU9W1X3t5ReAzcAJ/e1q6qplR3v14Parp5k10IEPkOSyJE8AHwcu7Xc/PfI7wLf63UQDnQA8MWZ9K3MwWAZZkqXAqcC/9reT6UkyL8n9wNPALVXV0+OY84Gf5NYkmzq8VgNU1SVVtRi4FpjVTzPf37G051wC7KZ1PLPSZI5jjkqHsTn7v8ZBk+QI4Abgj8f9737OqKo9VfUOWv+DX56kp5fbun2mbd9V1ZmTnPoN4Cbgcwewna7s71iSfApYBZxRs/jNlyn8mcw1W4HFY9YXAdv61IvGaF/zvgG4tqr+qd/9dKuqfpbkdmAF0LM31uf8Gf6+JFk2ZvVDwCP96qVbSVYAnwE+VFU7+91PQ90DLEtyYpL5wLnAhj731HjtNzu/Cmyuqr/sdz/TlWTotbvvkhwGnEmPM2vQ79K5AfiPtO4K+RFwQVX9uL9dTU+SLcAhwDPtobvn4h1HSc4BrgSGgJ8B91fVB/rb1eQlWQl8EZgHXF1Vl/W5pWlJ8k3gdFofw/sU8Lmq+mpfm5qmJO8B7gQeovVvHeDPqmpj/7qauiSnAF+n9XfrDcD/rqr/3tMagxz4kqSfG+hLOpKknzPwJakhDHxJaggDX5IawsCXpIYw8CWpIQx8SWqI/w8WaM5UOMRcLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X = np.array([[-1, -1], [3, 2]])\n",
    "#Y = np.array([1,  2])\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "Y2 = ['r', 'r', 'r', 'b', 'b', 'b']\n",
    "\n",
    "\n",
    "color = [ 'red' if y == 1 else 'blue' for y in Y]\n",
    "plt.scatter(X[:, 0], X[:, 1], color=color)\n",
    "\n",
    "\n",
    "t = np.array([[-0.8, -1]])\n",
    "plt.scatter(t[:,0], t[:,1], color='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.135274Z",
     "start_time": "2020-02-10T00:09:14.128293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2 1]\n",
      "\n",
      "\n",
      "['r']\n",
      "['b' 'b']\n"
     ]
    }
   ],
   "source": [
    "# 학습하기\n",
    "clf = GaussianNB()  \n",
    "clf.fit(X,Y)\n",
    "\n",
    "# 추정하기\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "print(clf.predict([[2.5, 1.3], [0,0]]))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# 학습하기\n",
    "clf.fit(X,Y2)\n",
    "\n",
    "# 추정하기\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "print(clf.predict([[2.5, 1.3], [0,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어, 일어, 영어, 중국어 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.139378Z",
     "start_time": "2020-02-10T00:09:14.136271Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습 전용 데이터 준비하기\n",
    "\n",
    "ko_str = '이것은 한국어 문장입니다.'\n",
    "ja_str = 'これは日本語 文章です。'\n",
    "en_str = 'This is English Sentences.'\n",
    "ch_str = '统一码'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.147280Z",
     "start_time": "2020-02-10T00:09:14.140261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이\n"
     ]
    }
   ],
   "source": [
    "print(ko_str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ord() \n",
    "> Return the Unicode code point for a one-character string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.154224Z",
     "start_time": "2020-02-10T00:09:14.148239Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51060\n",
      "12371\n",
      "84\n",
      "32479\n"
     ]
    }
   ],
   "source": [
    "#  숫자값들의 분포가 확연하게 다름을 알 수 있음\n",
    "print(ord(ko_str[0]))\n",
    "print(ord(ja_str[0]))\n",
    "print(ord(en_str[0]))\n",
    "print(ord(ch_str[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.160208Z",
     "start_time": "2020-02-10T00:09:14.155220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unicode 코드 포인트로 출현 빈도 판정하는 함수 만들기\n",
    "# 이건 글자의 위치에 관계 없는 함수 (예를들면, '홈런'과 '런홈'의 함수값이 똑같음)\n",
    "def count_codePoint(str):\n",
    "    \n",
    "    counter = np.zeros(65535)   # Unicode 코드 포인트를 저장할 배열 준비하기\n",
    "    \n",
    "    for i in range(len(str)):    \n",
    "        code_point = ord(str[i])   # 각 문자를 Unicode 코드 포인트로 변환하기\n",
    "        if code_point > 65535 :\n",
    "            continue\n",
    "            \n",
    "        counter[code_point] += 1    # 출현 횟수 세기\n",
    "        \n",
    "    counter = counter/len(str)     # 각 요소를 문자 수로 나눠 정규화하기\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.166257Z",
     "start_time": "2020-02-10T00:09:14.162202Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = [count_codePoint(ko_str),\n",
    "           count_codePoint(ja_str),\n",
    "           count_codePoint(en_str)]\n",
    "y_train = ['ko','ja','en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.172244Z",
     "start_time": "2020-02-10T00:09:14.167189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_codePoint(\"밝다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.179157Z",
     "start_time": "2020-02-10T00:09:14.173296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(array([   32,    46, 44163, 44397, 45768, 45796, 47928, 50612, 51008,\n",
      "       51060, 51077, 51109, 54620], dtype=int64),)\n",
      "\n",
      "\n",
      "[0.14285714 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      " 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      " 0.07142857]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])   # 이렇게 하면 잘 안보임\n",
    "print(np.where((x_train[0] > 0)))\n",
    "print('\\n')\n",
    "\n",
    "idx = np.where((x_train[0] > 0))\n",
    "data = x_train[0]\n",
    "\n",
    "print(data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB를 이용하여 언어 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:14.192122Z",
     "start_time": "2020-02-10T00:09:14.180153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하기\n",
    "clf = GaussianNB() \n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:43.374963Z",
     "start_time": "2020-02-10T00:09:43.367030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ko']\n"
     ]
    }
   ],
   "source": [
    "# 아무 언어도 없는 것도 평가가 된다????\n",
    "y_pred = clf.predict([count_codePoint(' ')])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:44.475971Z",
     "start_time": "2020-02-10T00:09:44.461961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'ja' 'ko']\n",
      "정답률 =  1.0\n"
     ]
    }
   ],
   "source": [
    "# 평가 전용 데이터 준비하기\n",
    "ko_test_str = '안녕. 어디야'\n",
    "ja_test_str = 'こんにちは'\n",
    "en_test_str = 'Hello'\n",
    "x_test = [count_codePoint(en_test_str),count_codePoint(ja_test_str),count_codePoint(ko_test_str)]\n",
    "y_test = ['en', 'ja', 'ko']\n",
    "\n",
    "# 평가하기\n",
    "y_pred = clf.predict(x_test)\n",
    "print(y_pred)\n",
    "print(\"정답률 = \" , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:44.854143Z",
     "start_time": "2020-02-10T00:09:44.837183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'ja' 'ja']\n",
      "정답률 =  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 위의 셀과 다른 데이터로 실험하기\n",
    "ko_test_str = '안녕'\n",
    "ja_test_str = 'こんにちは'\n",
    "en_test_str = 'Hello'\n",
    "x_test = [count_codePoint(en_test_str),count_codePoint(ja_test_str),count_codePoint(ko_test_str)]\n",
    "y_test = ['en', 'ja', 'ko']\n",
    "\n",
    "# 평가하기\n",
    "y_pred = clf.predict(x_test)\n",
    "print(y_pred)\n",
    "print(\"정답률 = \" , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문자set이 동일한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:45.981448Z",
     "start_time": "2020-02-10T00:09:45.677263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train\\de_cat.txt\n",
      "./train\\de_dog.txt\n",
      "./train\\de_elephant.txt\n",
      "./train\\en_cat.txt\n",
      "./train\\en_dog.txt\n",
      "./train\\en_elephant.txt\n",
      "./train\\es_cat.txt\n",
      "./train\\es_dog.txt\n",
      "./train\\es_elephant.txt\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 준비하기 --- (*1)\n",
    "x_train = []\n",
    "y_train = []\n",
    "for file in glob.glob('./train/*.txt'): # glob.glob() : 파일들의 목록으로 리스트를 구성함\n",
    "    # 언어 정보를 추출하고 레이블로 지정하기 --- (*2)\n",
    "    print(file)\n",
    "    y_train.append(file[8:10]) # y_train에 de, en, es 등을 추출해냄\n",
    "    \n",
    "    # 파일 내부의 문자열을 모두 추출한 뒤 빈도 배열로 변환한 뒤 입력 데이터로 사용하기 --- (*3)\n",
    "    file_str = ''\n",
    "    for line in open(file, 'r', encoding='UTF8'):\n",
    "        file_str = file_str + line  # file에서 한 줄씩 추출해서 이어붙이는 작업\n",
    "    x_train.append(count_codePoint(file_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:46.007378Z",
     "start_time": "2020-02-10T00:09:45.989427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하기\n",
    "clf = GaussianNB() \n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:46.076783Z",
     "start_time": "2020-02-10T00:09:46.068805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de']\n"
     ]
    }
   ],
   "source": [
    "# 평가하기\n",
    "y_pred = clf.predict([count_codePoint('hello. my name is ')])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 단어 빈도수 기반 자연어 처리\n",
    "- 단어 빈도수 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:47.098343Z",
     "start_time": "2020-02-10T00:09:47.093402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he12345\n",
      "he:123:45\n"
     ]
    }
   ],
   "source": [
    "a = [\"he\", \"123\", \"45\"]\n",
    "b = \"\".join(a)\n",
    "print(b)\n",
    "\n",
    "c = \":\".join(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:47.281021Z",
     "start_time": "2020-02-10T00:09:47.278075Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:47.783640Z",
     "start_time": "2020-02-10T00:09:47.778699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you know I want your love I like you what should I do\n",
      "words =  ['you', 'know', 'I', 'want', 'your', 'love', 'I', 'like', 'you', 'what', 'should', 'I', 'do']\n",
      "\n",
      "\n",
      "freq =  {'you': 2, 'know': 1, 'I': 3, 'want': 1, 'your': 1, 'love': 1, 'like': 1, 'what': 1, 'should': 1, 'do': 1}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "str = \" \".join(corpus)\n",
    "print(str)\n",
    "\n",
    "words = str.split(\" \")   # parsing\n",
    "print('words = ', words)\n",
    "print('\\n')\n",
    "\n",
    "freq = {}               # dictionary는 반드시 초기화 해준 상태에서 해야함\n",
    "for word in words:\n",
    "    freq[word] = freq.get(word, 0 ) + 1    # ferq.get(word, 0) : freq[word]가 정의가 안되어 있으면 0으로 출력하는 것!\n",
    "    \n",
    "print('freq = ', freq)\n",
    "print(freq[\"you\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    ">CountVectorizer는 다음과 같은 세가지 작업을 수행한다.\n",
    "- 문서를 토큰 리스트로 변환한다.\n",
    "- 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "- 각 문서를 BoW(Bag of Words) 인코딩 벡터로 변환한다.\n",
    "- 순서는 상관없음(바로 밑의 셀에서 what should와 should what을 비교해보면 앎)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer는 이러한 작업을 하기 위한 다음과 같은 인수를 가질 수 있다.\n",
    "\n",
    "- stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "    - stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "- analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "    - 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "- token_pattern : string\n",
    "    - 토큰 정의용 정규 표현식\n",
    "- tokenizer : 함수 또는 None (디폴트)\n",
    "    - 토큰 생성 함수 .\n",
    "- ngram_range : (min_n, max_n) 튜플\n",
    "    - n-그램 범위\n",
    "- max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "    - 단어장에 포함되기 위한 최대 빈도\n",
    "- min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "    - 단어장에 포함되기 위한 최소 빈도\n",
    "\n",
    "출처 : [데이터 사이언스 스쿨](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:51.721466Z",
     "start_time": "2020-02-10T00:09:51.717453Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'you know I want your love love love',\n",
    "    'I like you like',\n",
    "    'what should',\n",
    "    'should what'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:51.981720Z",
     "start_time": "2020-02-10T00:09:51.974720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 6, 'know': 0, 'want': 4, 'your': 7, 'love': 2, 'like': 1, 'what': 5, 'should': 3}\n",
      "  (0, 6)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 2)\t3\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 3)\t1\n",
      "\n",
      "\n",
      "[[1 0 3 0 1 0 1 1]\n",
      " [0 2 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 0]\n",
      " [0 0 0 1 0 1 0 0]]\n",
      "\n",
      "\n",
      "(4, 8)\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer()\n",
    "# print(vector)  # 이건 별 정보 아님\n",
    "\n",
    "tf = vector.fit_transform(corpus) # 각 문장을 counter vector로 변환(구체적으로는 tf.toarray())\n",
    "print(vector.vocabulary_) # 각 단어마다 부여한 인덱스를 표시함        # 'I' 같이 짧은 단어는 제외됌\n",
    "\n",
    "print(tf)     # (문장index, 단어 index) -  문장에서 단어가 나온 횟수\n",
    "print('\\n')\n",
    "\n",
    "print(tf.toarray())  # sparse matrix 표현으로\n",
    "print('\\n')\n",
    "\n",
    "print(tf.shape)  # (문장 총 갯수, 인식된 단어 총 갯수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:52.194727Z",
     "start_time": "2020-02-10T00:09:52.190782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 6, 'know': 0, 'want': 4, 'your': 7, 'love': 2, 'like': 1, 'what': 5, 'should': 3}\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(vector.vocabulary_)  # 각 단어마다 부여한 인덱스를 표시함  # 'I' 같이 짧은 단어는 제외됌\n",
    "print(vector.vocabulary_['you'])  # 'you'라는 단어의 index\n",
    "print(vector.vocabulary_.get('you')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:52.477172Z",
     "start_time": "2020-02-10T00:09:52.471233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['know', 'like', 'love', 'should', 'want', 'what', 'you', 'your']\n",
      "know\n",
      "like\n",
      "love\n",
      "should\n",
      "want\n",
      "what\n",
      "you\n",
      "your\n",
      "\n",
      "\n",
      "you 6\n",
      "know 0\n",
      "want 4\n",
      "your 7\n",
      "love 2\n",
      "like 1\n",
      "what 5\n",
      "should 3\n"
     ]
    }
   ],
   "source": [
    "words = vector.get_feature_names()     # 어휘집 생산\n",
    "print(words)\n",
    "for word in words : print(word)\n",
    "print('\\n')\n",
    "\n",
    "for key in vector.vocabulary_:   # 각 단어와 그 index를 출력\n",
    "    print(key, vector.vocabulary_[key]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency - Inverse Documnet Frequency)\n",
    "- $tF_{x,y}$ : frequency of x in y (matrix)\n",
    "- $dF_{x}$ : number of documents containing x\n",
    "- **IDF** means the inverse of DF\n",
    "- $N$ : total number of documents\n",
    "\n",
    "$\\Longrightarrow \\hspace{0.5cm} TF-IDF$ is defined as \n",
    "\\begin{align}\n",
    "TF-IDF = tF_{x,y} \\times log\\left(\\frac{N}{1 + dF_{x}} \\right)\n",
    "\\end{align}\n",
    "\n",
    "- 특정 단어의 상대적인 빈도를 나타내주는 값으로\n",
    "    - $\\huge{값이} \\hspace{0.2cm} \\huge{클} \\hspace{0.2cm} \\huge{수록}$ 내 문서에만 많이 언급되는 단어(= 다른 문서에서는 잘 언급 안됨)\n",
    "    - $\\huge{값이} \\hspace{0.2cm} \\huge{작을} \\hspace{0.2cm} \\huge{수록}$ 다른 문서에 잘 언급하는 단어를 의미(=현재 문서와 관련 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main parameters in Scikit-learn source code\n",
    "1.  sublinear-tf = false(default)\n",
    "    - tf : the number of docutment \n",
    "\n",
    "\n",
    "2.  smooth-idf = True(default)\n",
    "    - idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1\n",
    "\n",
    "\n",
    "3.  norm = l2(default)\n",
    "    - Sum of squares of vector elements is 1.\n",
    "\n",
    "\n",
    "**Reference**:\n",
    "\n",
    "[github.com/scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/b194674c42d54b26137a456c510c5fdba1ba23e0/sklearn/feature_extraction/text.py#L1329)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:53.661423Z",
     "start_time": "2020-02-10T00:09:53.653490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t0.6437444595062429\n",
      "  (0, 7)\t0.7652405313723362\n",
      "  (1, 17)\t0.6437444595062429\n",
      "  (1, 7)\t0.7652405313723362\n",
      "  (2, 12)\t0.28487998702172107\n",
      "  (2, 6)\t0.28487998702172107\n",
      "  (2, 4)\t0.28487998702172107\n",
      "  (2, 1)\t0.28487998702172107\n",
      "  (2, 9)\t0.35310140100264525\n",
      "  (2, 14)\t0.28487998702172107\n",
      "  (2, 8)\t0.35310140100264525\n",
      "  (2, 13)\t0.35310140100264525\n",
      "  (2, 5)\t0.35310140100264525\n",
      "  (2, 17)\t0.19893117008503197\n",
      "  (2, 7)\t0.23647612349029334\n",
      "  (3, 11)\t0.3542556015420614\n",
      "  (3, 16)\t0.3542556015420614\n",
      "  (3, 3)\t0.3542556015420614\n",
      "  (3, 10)\t0.3542556015420614\n",
      "  (3, 0)\t0.3542556015420614\n",
      "  (3, 2)\t0.3542556015420614\n",
      "  (3, 15)\t0.3542556015420614\n",
      "  (3, 14)\t0.28581118874948447\n",
      "  (3, 17)\t0.1995814265359179\n",
      "  (4, 12)\t0.5\n",
      "  (4, 6)\t0.5\n",
      "  (4, 4)\t0.5\n",
      "  (4, 1)\t0.5\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "sent = [\"오늘 휴일\", \n",
    "        \"휴일 오늘\", \n",
    "        \"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \n",
    "        \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다.\", \n",
    "        \" 내일 은 반가운 비 소식 이 있습니다.\"] \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sent) #각 문장을 성분이 \n",
    "print(tfidf_matrix)\n",
    "print(type(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:54.298365Z",
     "start_time": "2020-02-10T00:09:54.293388Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 18)\n",
      "(5, 18)\n",
      "\n",
      "\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.76524053 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.64374446]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.76524053 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.64374446]\n",
      " [0.         0.28487999 0.         0.         0.28487999 0.3531014\n",
      "  0.28487999 0.23647612 0.3531014  0.3531014  0.         0.\n",
      "  0.28487999 0.3531014  0.28487999 0.         0.         0.19893117]\n",
      " [0.3542556  0.         0.3542556  0.3542556  0.         0.\n",
      "  0.         0.         0.         0.         0.3542556  0.3542556\n",
      "  0.         0.         0.28581119 0.3542556  0.3542556  0.19958143]\n",
      " [0.         0.5        0.         0.         0.5        0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)    # (문장 총 갯수, 인식된 단어 총 갯수)\n",
    "print(tfidf_matrix.toarray().shape)   #  위랑 똑같음\n",
    "print('\\n')\n",
    "\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:55.113996Z",
     "start_time": "2020-02-10T00:09:55.110028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['갑작스런', '내일', '놀러왔다가', '망연자실', '반가운', '서쪽', '소식', '오늘', '으로', '이어졌는데요', '인해', '있습니', '있습니다', '중심', '폭염', '피해서', '하고', '휴일']\n"
     ]
    }
   ],
   "source": [
    "features = tfidf_vectorizer.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:36:48.863907Z",
     "start_time": "2020-02-07T05:36:48.860878Z"
    }
   },
   "source": [
    "### tfidf_vectorizer.get_feature_names 이건 한글전용 인식기가 아니어서 '은', '이' 같이 한 글자짜리는 인식을 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:56.267778Z",
     "start_time": "2020-02-10T00:09:56.261749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 17]\n",
      "\n",
      "\n",
      "[[0.76524053 0.64374446]\n",
      " [0.76524053 0.64374446]\n",
      " [0.23647612 0.19893117]\n",
      " [0.         0.19958143]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mat = np.asarray(tfidf_matrix.toarray())\n",
    "srch = [\"오늘\", \"휴일\"]\n",
    "srch_dtm = mat[:, [ tfidf_vectorizer.vocabulary_.get(i) for i in srch]]\n",
    "print([tfidf_vectorizer.vocabulary_.get(i) for i in srch]) # '오늘'과 '휴일'이라는 단어의 인덱스 값들\n",
    "print('\\n')\n",
    "\n",
    "print(srch_dtm) # 첫번째 문장에서 '오늘'과 '휴일'의 TF는 같지만 IDF쪽이 달라서 첫번째 row의 수치가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:56.475024Z",
     "start_time": "2020-02-10T00:09:56.469036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40898499 1.40898499 0.43540729 0.19958143 0.        ]\n",
      "\n",
      "\n",
      "오늘 휴일 /score : 1.408984990878579\n",
      "휴일 오늘 /score : 1.408984990878579\n",
      "휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다. /score : 0.4354072935753253\n",
      "폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다. /score : 0.1995814265359179\n"
     ]
    }
   ],
   "source": [
    "score = srch_dtm.sum(axis=1)\n",
    "print(score)\n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(score)):\n",
    "    if score[i] > 0:\n",
    "        print('{} /score : {}'.format(sent[i], score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:56.680978Z",
     "start_time": "2020-02-10T00:09:56.675953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 14]\n",
      "\n",
      "\n",
      "[[0.76524053 0.        ]\n",
      " [0.76524053 0.        ]\n",
      " [0.23647612 0.28487999]\n",
      " [0.         0.28581119]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mat = np.asarray(tfidf_matrix.toarray())\n",
    "srch = [\"오늘\", \"폭염\"]\n",
    "srch_dtm = mat[:, [ tfidf_vectorizer.vocabulary_.get(i) for i in srch]]\n",
    "print([tfidf_vectorizer.vocabulary_.get(i) for i in srch])\n",
    "print('\\n')\n",
    "\n",
    "print(srch_dtm) # 첫번째 문장에서 '오늘'과 '폭염'의 TF는 같지만 IDF쪽이 달라서 첫번째 row의 수치가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:56.981482Z",
     "start_time": "2020-02-10T00:09:56.975498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76524053 0.76524053 0.52135611 0.28581119 0.        ]\n",
      "\n",
      "\n",
      "오늘 휴일 /score : 0.7652405313723362\n",
      "휴일 오늘 /score : 0.7652405313723362\n",
      "휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다. /score : 0.5213561105120144\n",
      "폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다. /score : 0.28581118874948447\n"
     ]
    }
   ],
   "source": [
    "score = srch_dtm.sum(axis=1)\n",
    "print(score)\n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(score)):\n",
    "    if score[i] > 0:\n",
    "        print('{} /score : {}'.format(sent[i], score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:57.634741Z",
     "start_time": "2020-02-10T00:09:57.628758Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = ['This is the first document.',\n",
    "              'This is the second document.',\n",
    "              'And the third one.',\n",
    "              'Is this the first document?']\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:57.880589Z",
     "start_time": "2020-02-10T00:09:57.869543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='perceptron',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1,2,3,4]\n",
    "model = SGDClassifier(loss='perceptron')\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T00:09:58.105941Z",
     "start_time": "2020-02-10T00:09:58.100993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "X_pred = vect.transform(['My new document third'])\n",
    "y_pred = model.predict(X_pred)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
